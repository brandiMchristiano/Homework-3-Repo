---
title: 'Brandi Christiano - Homework #3 Notebook'
author: "BMC"
date: "4/21/2021"
output: html_document
---

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(tidyverse)
library(tidyr)
library (ggpmisc)
library(dplyr)
library(broom)
library(infer)
library(mosaic)
```

```{r setup, include=FALSE}
df <- read.csv("KamilarAndCooperData.csv")
```

```{r}
head(df)
```


# **Challenge 1**

## Normal Model (Model1)

### Fit the regression model 
```{r}
# Normal Model
model1 <- lm(WeaningAge_d~Brain_Size_Species_Mean, data=df)
model1
```
###  Using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data
```{r, warning=FALSE}
# x is the control variable
plot1 <- ggplot(data = df, aes(x =(Brain_Size_Species_Mean), y =(WeaningAge_d))) + geom_point() + geom_smooth(method = "lm", formula = y ~ x) 
plot1
```


### Append the the fitted model equation to your plot.
```{r, warning=FALSE}
# Make the output of Model 1 an Equation to include on the plot 

eq <- substitute(italic(y) == a + b *"x" , 
         list(a = format(unname(coef(model1)[1]), digits = 2),
              b = format(unname(coef(model1)[2]), digits = 2)))
eq <- c("y == 132+2.6(x)")
eq

plot1 <- plot1+ geom_text(x = 400, y = 1500, label =eq, parse = TRUE)

plot1
```

### Identify and interpret the point estimate of the slope (β1)
 
```{r}
# Model 1 Betas 
model.summary <- tidy(model1)

beta0 <- model.summary %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)

beta1 <- model.summary %>%
  filter(term == "Brain_Size_Species_Mean") %>%
  pull(estimate)

beta0
beta1

# INTERPERTATION: Beta 1 is the true slope, therefore every time the  x variable (Mean of Brain Size) increases by 1, the y variable (Weaning Age in Days), will increase by Beta 1, which is 2.64. 

# Test null hypothesis that beta1 = 0
beta1==0 

# Test alternative hypothesis that beta1 does not = 0
beta1!=0 

```
### Find a 90% CI for the slope (β1) parameter
```{r}
# Model 1 CIs
alpha <- 0.25
ci <- predict(model1,
  newdata = data.frame(Brain_Size_Species_Mean = 2.64),
  interval = "confidence", level = 1 - alpha
) # for a single value
ci

```
### Using your model, add lines for the 90% confidence
```{r, warning=FALSE}
# Model 1

df2 <- augment(model1, se_fit = TRUE)
head(df2)


d <- mutate(df, centered_Age = WeaningAge_d - mean(WeaningAge_d))
d <- mutate(df, centered_Brain = Brain_Size_Species_Mean - mean(Brain_Size_Species_Mean))

newdata <-  data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean)


ci <- predict(model1,newdata,
              interval = "confidence", level = 1 - alpha) 
ci <- data.frame(ci)
ci <- cbind(df$Brain_Size_Species_Mean, ci)
names(ci) <- c("Brain.Size.Species.Mean", "c.fit", "c.lwr", "c.upr")
g <- ggplot(data = df, aes(x = Brain_Size_Species_Mean, y = WeaningAge_d))
g <- g + geom_point(alpha = 0.1)
g2 <- g + geom_line(
  data = ci, aes(x =ci$Brain.Size.Species.Mean, y = c.fit),
  color = "black"
)
g3 <- g2 + geom_line(
  data = ci, aes(x =ci$Brain.Size.Species.Mean , y = c.lwr),
  color = "blue"
)
g4 <- g3 + geom_line(
  data = ci, aes(x = ci$Brain.Size.Species.Mean, y = c.upr),
  color = "blue"
)
g4

```


### Using your model, add lines for prediction interval bands on the plot
```{r, warning=FALSE}
pi <- predict(model1, newdata ,
  interval = "prediction", level = 0.90
) # for a vector of values
pi <- data.frame(pi)

pi <- cbind(df$Brain_Size_Species_Mean, pi)
names(pi) <- c("Brain.Size.Species.Mean", "p.fit", "p.lwr", "p.upr")

g5 <- g4 + geom_line(data = pi, aes(x = Brain.Size.Species.Mean, y = p.lwr), color = "red")
g5
g6 <- g5 + geom_line(data = pi, aes(x = Brain.Size.Species.Mean, y = p.upr), color = "red")
g6 
```


### Produce a point estimate for the weaning age of a species whose brain weight is 750 mg. 

```{r}
# Normal Model
beta0 <- model.summary %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
beta1 <- model.summary %>%
  filter(term == "Brain_Size_Species_Mean") %>%
  pull(estimate)
(h.hat <- beta1 * 750 + beta0)
```
### Produce the  associated 90% prediction interval for the weaning age of a species whose brain weight is 750 mg.
```{r}
# Normal Model
pi <- predict(model1,
  newdata = data.frame(Brain_Size_Species_Mean = 750),
  interval = "prediction", level = 0.90
) # for a single value
pi

```






















## Log Model (Model2)

### Fit the regression model 
```{r, warning=FALSE}
# Log Model
model2 <- lm(log(WeaningAge_d)~log(Brain_Size_Species_Mean), data=df)
model2
```
###  Using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data
```{r, warning=FALSE}
# Log Model Plot 
plot2 <- ggplot(data = df, aes(x = log(Brain_Size_Species_Mean), y = log(WeaningAge_d))) + geom_point() + geom_smooth(method = "lm", formula = y ~ x) 
```
### Append the the fitted model equation to your plot.
```{r, warning=FALSE}
eq2 <- substitute(italic(y) == a + b *"x" , 
         list(a = format(unname(coef(model2)[1]), digits = 2),
              b = format(unname(coef(model2)[2]), digits = 2)))
eq2 <- c("y == 3.4+0.56(x)")


plot2 <- plot2+ geom_text(x = 2, y = 6.5, label =eq2, parse = TRUE)

plot2
```

### Identify and interpret the point estimate of the slope (β1)
```{r}
# Log Model
model.summary2 <- tidy(model2)

beta0 <- model.summary2 %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)

beta1 <- model.summary2 %>%
  filter(term == "log(Brain_Size_Species_Mean)") %>%
  pull(estimate)

beta0 
beta1

# INTERPERTATION: Beta 1 is the true slope, therefore every time the  x variable (the log of the mean of brain size) increases by 1, the y variable (the log of weaning age in days), will increase by Beta 1, which is 0.57. 

# Test null hypothesis that beta1 = 0
beta1==0 

# Test alternative hypothesis that beta1 does not = 0
beta1!=0 
```
###  Find a 90% CI for the slope (β1) parameter.
```{r, warning=FALSE}
# Model 2 CIs
alpha <- 0.25
ci2 <- predict(model2,
  newdata = data.frame(Brain_Size_Species_Mean = 0.57),
  interval = "confidence", level = 1 - alpha
) # for a single value
ci2

df3 <- augment(model2, se_fit = TRUE)


d <- mutate(df, centered_Age = log(WeaningAge_d) - mean(log(WeaningAge_d)))
d <- mutate(df, centered_Brain = log(Brain_Size_Species_Mean) - mean(log(Brain_Size_Species_Mean)))


newdata <-  data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean)


ci <- predict(model2,newdata,
              interval = "confidence", level = 1 - alpha) 


ci <- data.frame(ci)


BrainLog <- log(df$Brain_Size_Species_Mean)

ci <- cbind(BrainLog, ci)

names(ci) <- c("Log.Brain.Size.Species.Mean", "c.fit", "c.lwr", "c.upr")

g <- ggplot(data = df, aes(x = log(Brain_Size_Species_Mean), y = log(WeaningAge_d)))
g <- g + geom_point(alpha = 0.1)
g2 <- g + geom_line(
  data = ci, aes(x =ci$Log.Brain.Size.Species.Mean, y = c.fit),
  color = "black"
)
g2
g3 <- g2 + geom_line(
  data = ci, 
  aes(x =ci$Log.Brain.Size.Species.Mean, y = c.lwr),
  color = "blue"
)
g3
g4 <- g3 + geom_line(
  data = ci, aes(x = ci$Log.Brain.Size.Species.Mean, y = c.upr),
  color = "blue"
)
g4
```

### Using your model, add lines for prediction interval bands on the plot

```{r, warning=FALSE}
pi <- predict(model2, newdata ,
  interval = "prediction", level = 0.90
) # for a vector of values
pi <- data.frame(pi)

pi <- cbind(BrainLog, pi)
names(pi) <- c("Log.Brain.Size.Species.Mean", "p.fit", "p.lwr", "p.upr")

g5 <- g4 + geom_line(data = pi, aes(x = Log.Brain.Size.Species.Mean, y = p.lwr), color = "red")
g6 <- g5 + geom_line(data = pi, aes(x = Log.Brain.Size.Species.Mean, y = p.upr), color = "red")
g6 

```



### Produce a point estimate for the weaning age of a species whose brain weight is 750 gm.
```{r}
# Log Model
log(750) 
beta0 <- model.summary2 %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
beta0
beta1 <- model.summary2 %>%
  filter(term == "log(Brain_Size_Species_Mean)") %>%
  pull(estimate)
beta1
(h.hat <- beta1 * 6.62 + beta0)

```

### Produce the  associated 90% prediction interval for the weaning age of a species whose brain weight is 750 gm.
```{r}
#Log Model
pi2 <- predict(model2,
  newdata = data.frame(Brain_Size_Species_Mean = 6.62),
  interval = "prediction", level = 0.90
) # for a single value
pi2
```

###  Q - Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
### A -  No, I think the variables are too skewed for the normal model to predict an accurate observation. I would trust the log transformations more. 

### Q - Looking at your two models (i.e., untransformed versus log-log transformed), which do you think is better? Why?
### A -  Because the variables in the untransformed model are highly skewed, I think the log-transformed model would be better. The log-transformation creates a  more normalized data set and results in a more linear relationship. 


# **Challenge 2**
### Run a linear regression looking at log(MeanGroupSize) in relation to log(Body_mass_female_mean) 
```{r}
model3 <-  lm(log(MeanGroupSize)~ log(Body_mass_female_mean), data=df)
model3
```


### Report your  β  coeffiecients (slope and intercept).
```{r}
model.summary3 <- tidy(model3)
beta0 <- model.summary3 %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
beta0 
beta1 <- model.summary3 %>%
  filter(term == "log(Body_mass_female_mean)") %>%
  pull(estimate)
beta1 
```

### Use bootstrapping to sample from the dataset 1000 times with replacement, each time fitting the same model and calculating the appropriate coefficients. [The size of each sample should be equivalent to the total number of observations in the dataset.] 
```{r , warning=FALSE}
# Permutes the slope


logGroup <- log(df$MeanGroupSize)
logFMass <- log(df$Body_mass_female_mean)

df2 <- cbind(logGroup,logFMass)
df2 <- as.data.frame(df2)

permuted.slope <- df2 %>%
  specify(logGroup ~ logFMass) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  # calculate the slope statistic
  calculate(stat = "slope")

intercept <-  mean(df2$logGroup, na.rm=T)-permuted.slope$stat*(mean(df2$logFMass, na.rm = T))

hist(permuted.slope$stat, main="Permuted Slopes (B1)")
hist(intercept, main= "Permuted Intercepts (B0)")
```

### Estimate the standard error for each of your β oefficients as the standard deviation of the sampling distribution from your bootstrap.

```{r}
# SE of permuted slopes
mean.perm.slope <- mean(permuted.slope$stat)
sd.perm.slope <- sd(permuted.slope$stat)
SE.slope <- sd.perm.slope / sqrt(length(permuted.slope))
SE.slope

#SE of permuted intercepts
mean.intercept <- mean(intercept)
sd.intercept <- sd(intercept)

SE <-  sd.intercept / sqrt(length(intercept))
SE
```

### Determine the 95% CI for each of your  β  coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
# Beta1 
percent_ci <- 95
alpha <- 1 - percent_ci / 100 # alpha = 0.05
lower <- mean.perm.slope + qnorm(alpha / 2) * SE.slope
upper <- mean.perm.slope + qnorm(1 - alpha / 2) * SE.slope
(ci <- c(lower, upper))

```

```{r}
# Beta0
percent_ci <- 95
alpha <- 1 - percent_ci / 100 # alpha = 0.05
lower <- mean.intercept + qnorm(alpha / 2) * SE
upper <- mean.intercept + qnorm(1 - alpha / 2) * SE
(ci <- c(lower, upper))
```

### Q - How do the SEs estimated from the bootstrap sampling distribution compare to those estimated mathematically as part of lm() function?

### A -  The standard error of my bootstrap distribution is 0.02 which is much smaller than the standard error estimated by the linear model function, which was 0.4. 


### Q - How do your bootstrap CIs compare to those estimated mathematically as part of the lm() function?

### A - My bootstrapped CIs have a wider range than those estmiated from the lm function. 




# **Challenge 3**


### Write your own function, called boot_lm(), that takes as its arguments a dataframe (d=), a linear model (model=, written as a character string, e.g., “logGS ~ logBM”), a user-defined confidence interval level (conf.level=, with default “0.95”), and a number of bootstrap replicates (reps=, with default “1000”).


```{r}
boot_lm <- function(data=df, y,x,conf.level=0.95, reps=1000) {
  model<- lm(log(y)~ log(x))
   dff=data.frame(model$coefficients[1],model$coefficients[2],model$coefficients[3], model$coefficients[4] )
    colnames(dff) <-  c("Beta0 Normal Model","Beta1 Normal Model","Beta0 SE Normal Model", "Beta1 SE Normal Model")
    alpha <- 0.05
    percent_ci <- 95
    alpha <- 1-percent_ci / 100
    lower <- mean(model$coefficients[1]) + qnorm(alpha/2) * model$coefficients[2]
    upper <- mean(model$coefficients[1])+ qnorm(1- alpha / 2) * model$coefficients[2]
    ci <- cbind(lower,upper)
    dff <-  cbind(dff,ci)
   
    beta.0.BS <- numeric(reps)
    beta.1.BS <- numeric(reps)
    beta.0.SE.BS <- numeric(reps)
    beta.1.SE.BS <- numeric(reps)
    
      for (i in 1:reps) {
        model<- lm(log(y)~ log(x))
     
    percent_ci <- 95
    alpha <- 1-percent_ci / 100
    lower <- mean(model$coefficients[1]) + qnorm(alpha/2) * model$coefficients[2]
    upper <- mean(model$coefficients[1])+ qnorm(1- alpha / 2) * model$coefficients[2]
    ci <- cbind(lower,upper)
    dff1 <-  cbind(dff,ci)
    
    beta.0.BS[i] <- (model$coefficients[1])
    beta.1.BS[i] <- (model$coefficients[2])
    beta.0.SE.BS [i] <- (model$coefficients[3])
    beta.1.SE.BS[i] <- (model$coefficients[4])
    
    m1 <- mean(beta.0.BS)
    m2 <- mean(beta.1.BS)
    m3 <- mean(beta.0.SE.BS)
    m4 <- mean(beta.1.SE.BS)
    df.means <- data.frame(m1,m2,m3,m4)
     colnames(df.means) <-  c("Bootstrapped Beta0","Bootstrapped Beta1","Bootstrapped Beta0 SE", "Bootstrapped Beta1 SE")
    dff3 <- cbind(dff,df.means)
    return(dff3)
      }}

```

```{r}

boot1 <- boot_lm(data=df, y=df$MeanGroupSize, x=df$Body_mass_female_mean) 
boot1

boot2 <- boot_lm(data=df, y=df$DayLength_km, x=df$Body_mass_female_mean) 
boot2
  
boot3 <- boot_lm(df, df$DayLength_km, (df$Body_mass_female_mean)* (df$MeanGroupSize)) 





